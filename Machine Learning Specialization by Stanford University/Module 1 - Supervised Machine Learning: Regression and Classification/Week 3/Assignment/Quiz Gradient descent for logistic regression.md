# Practice quiz: Gradient descent for logistic regression
> ### 1.
> 
> Question 1
> 
> ![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Tv99olQhR7K_faJUIbeyDQ_61ca3d07977d46b4aec7eae43554bca1_Screen-Shot-2022-06-29-at-8.41.24-PM.png?expiry=1658620800000&hmac=yHhppXLkJD4ZMOx-ZEAyNmSoZcM6wkTkPUyFTfFE06o)
> 
> Which of the following two statements is a more accurate statement about gradient descent for logistic regression?
> 
> 1 / 1 point
> 

      The update steps look like the update steps for linear regression, but the definition of fw⃗,b(x(i))f_{\vec{w},b}(\mathbf{x}^{(i)})fw,b​(x(i)) is different. 
> 
>  The update steps are identical to the update steps for linear regression. 
> 
> Correct
> 
> For logistic regression, fw⃗,b(x(i))f_{\vec{w},b}(\mathbf{x}^{(i)})fw,b​(x(i)) is the sigmoid function instead of a straight line.
>
> -- https://www.coursera.org/learn/machine-learning/exam/XqVmD/practice-quiz-gradient-descent-for-logistic-regression/view-attempt#tqRxMT-legend
