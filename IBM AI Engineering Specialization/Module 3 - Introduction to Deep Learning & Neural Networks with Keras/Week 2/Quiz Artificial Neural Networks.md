# Artificial Neural Networks
> 
> Latest Submission Grade
> 
> 100%
> 
>  1.Question 1
> 
> The weights and biases in a neural network are optimized using:
> 
> 1 / 1 point 
> 
>  Activation Function 
> 

      Gradient Descent 
> 
>  Activation Descent 
> 
>  Vanishing Gradient 
> 
>  Logistic Descent 
> 
> Check
> 
> Correct
> 
> Correct
> 
>  2.Question 2
> 
> For a cost function, J=∑i=1m(zi−wxi−b)2J = \sum_{i=1}^{m}{(z_{i} - wx_{i} - b)^2} J=∑i=1m​(zi​−wxi​−b)2, that we would like to minimize, which of the following expressions represent updating the parameter, ww w, using gradient descent?
> 
> 1 / 1 point 
> 
>  w→w+b−η∗∂J∂ww \rightarrow w + b - \eta * \frac{\partial J}{\partial w} w→w+b−η∗∂w∂J​ 
> 
>  w→w+η∗∂J∂ww \rightarrow w + \eta * \frac{\partial J}{\partial w} w→w+η∗∂w∂J​ 
> 

    >  w→w−η∗∂J∂w
> 
>  w→w−η∗x∂J∂ww \rightarrow w - \eta * x\frac{\partial J}{\partial w} w→w−η∗x∂w∂J​ 
> 
>  w→w−η∗b∂J∂ww \rightarrow w - \eta * b\frac{\partial J}{\partial w} w→w−η∗b∂w∂J​ 
> 
> Check
> 
> Correct
> 
> Correct
> 
>  3.Question 3
> 
> What type of activation function is this?
> 
> ![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/HoF30b0IEemdqA7UJJ_tgg_618449fbc19c7a3d67e5759806a654eb_Relu_activation_function.png?expiry=1594857600000&hmac=ntTv-NXC6QtxmlvKeQC8G8H0rXcPmSIlw1C3I-hq3EQ)
> 
> 1 / 1 point 
> 
>  Linear Function 
> 
>  Hyperbolic Tangent Function 
> 

      ReLU 
> 
>  Leaky ReLU 
> 
>  Binary Function 
> 
>  Sigmoid Function 
> 
> Check
> 
> Correct
> 
> Correct
> 
>  4.Question 4
> 
> What type of activation function is this?
> 
> ![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/qwF5Wr0EEemNdQ5h0qdR0A_e5dda4cda46cb3dc3027e1e5958d36a4_tanh_activation_function.png?expiry=1594857600000&hmac=waVTbnlxaK2Y1L46OxLzlJqgQZVWtv-3xJWhDQlMtYg)
> 
> 1 / 1 point 
> 
>  ReLU 
> 
>  Sigmoid Function 
> 

      Hyperbolic Tangent Function 
> 
>  Leaky ReLU 
> 
>  Linear Function 
> 
>  Binary Function 
> 
> Check
> 
> Correct
> 
> Correct
> 
>  5.Question 5
> 
> Softmax activation function is most commonly used in hidden layers?
> 
> 1 / 1 point 
> 
>  True 
> 

      False 
> 
> Check
> 
> Correct
> 
> Correct.
>
> -- https://www.coursera.org/learn/introduction-to-deep-learning-with-keras/exam/gy8Q3/artificial-neural-networks/attempt?redirectToCover=true#Tunnel Vision Close
