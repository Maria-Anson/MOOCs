## Week 4 overview

*****

Welcome to the fourth week of the "How to Win a Data Science Competition"
course. Here is a short summary of what you will learn.

* *Dmitry Ulyanov  *will start this week with **hyperparameter optimization**. We
will understand hyperparameter tuning process in general, list most important
hyperparameters for major machine learning models and describe their impact. We
will also have a special video with **practical tips and tricks**, recorded by
four instructors.

* *Mikhail Trofimov *and* Dmitry Altukhov *will discuss **advanced features** that
we can extract from the data. We will describe matrix factorization technique
for feature extraction, learn to create features based on t-SNE, review concept
of feature interactions, and learn to make up new features based on statistics
and nearest neighbors. If you fill that you can do more we recommend you to take
a look at , where you will need to implement features based on nearest
neighbors, which are often could provide an edge in a competition.

* *Marios Michailidis*, kaggle top-1,will review **ensembling methods**. We will
describe and compare ensembling methods such as weighted averaging, bagging,
boosting, stacking. We will outline plan of validation schemes, discuss
practical tips and tricks, and implement ensembling in the .

To keep up the work on the , apply ideas we will discuss and improve your
current solution by optimizing your hyperparameters, maybe implementing some of
the advanced features and for sure trying out ensembling.

Now let's go ahead and get started!
