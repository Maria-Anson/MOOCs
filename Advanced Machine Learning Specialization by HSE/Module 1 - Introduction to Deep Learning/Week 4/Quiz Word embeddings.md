# Word embeddings
> 
> Total points 4
> 
>  1.Question 1
> 
> Which of the following is true about word2vec model?
> 
> 1 point 
> 

      It requires some text corpora for training. 
> 
>  It has one trainable parameter per word. 
> 
>  It uses convolutional layers and pooling. 
> 
>  It's outputs (predictions) are linear functions of inputs. 
> 
>  It requires human-defined semantic relations between words. 
> 
>  2.Question 2
> 
> How can you train word2vec model?
> 
> 1 point 
> 
>  By changing order of words in the corpora. 
> 

      By learning to predict context (neighboring words) given one word. 
> 

      By minimizing crossentropy (aka maximizing likelihood). 
> 

      By learning to predict omitted word by it's context. 
> 

      By applying stochastic gradient descent. 
> 
>  By minimizing distance between human-defined synonyms and maximizing distance between antonyms. 
> 
>  3.Question 3
> 
> Here's an [online demo](http://bionlp-www.utu.fi/wv_demo/) of word2vec model. Let's use it to find synonyms for rare words**.**
> 
> Don't forget to choose English GoogleNews model.
> 
> Which of the following words is in top 10 synonyms for **"weltschmerz".**
> 
> 1 point 
> 
>  big_bang 
> 

      despair 
> 
>  worldbuilding 
> 
>  decrystalization 
> 
>  4.Question 4
> 
> Which of the following is an appropriate way to measure similarity between word vectors v1 and v2? (more = better)
> 
> 1 point 
> 

      -||v1 - v2|| 
> 

      cos(v1,v2) 
> 
>  ||v1 - v2|| 
> 
>  sin(v1,v2)
>
> -- https://www.coursera.org/learn/intro-to-deep-learning/exam/zYQEk/word-embeddings/attempt#Tunnel Vision Close
