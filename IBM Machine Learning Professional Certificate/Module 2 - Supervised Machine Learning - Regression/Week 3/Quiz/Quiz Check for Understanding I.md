# Check for Understanding
> ### 1.
> 
> Question 1
> 
> Which of the following statements about model complexity is TRUE?
> 
> 1 / 1 point
> 
>  Higher model complexity leads to a lower chance of overfitting. 
> 
      Higher model complexity leads to a higher chance of overfitting. 
> 
>  Reducing the number of features while adding feature interactions leads to a lower chance of overfitting. 
> 
>  Reducing the number of features while adding feature interactions leads to a higher chance of overfitting. 
> 
> Correct
> 
> Correct! You can find more information in the Bias Trade Off lesson.
> 
> ### 2.
> 
> Question 2
> 
> Which of the following statements about model errors is TRUE?
> 
> 1 / 1 point
> 
>  Underfitting is characterized by lower errors in both training and test samples. 
> 
>  Underfitting is characterized by higher errors in both training and test samples. 
> 
      Underfitting is characterized by higher errors in training samples and lower errors in test samples. 
> 
>  Underfitting is characterized by lower errors in training samples and higher errors in test samples. 
> 
> Correct
> 
> Correct! You can find more information in the Bias Trade Off lesson.
> 
> ### 3.
> 
> Question 3
> 
> Which of the following statements about regularization is TRUE?
> 
> 1 / 1 point
> 
>  Regularization always reduces the number of selected features. 
> 
>  Regularization increases the likelihood of overfitting relative to training data. 
> 
      Regularization decreases the likelihood of overfitting relative to training data. 
> 
>  Regularization performs feature selection without a negative impact in the likelihood of overfitting relative to the training data. 
> 
> Correct
> 
> Correct! You can find more information in the Regularization Techniques lesson.
> 
> ### 4.
> 
> Question 4
> 
> BOTH Ridge regression and Lasso regression
> 
> 1 / 1 point
> 
>  do not adjust the cost function used to estimate a model. 
> 
      add a term to the loss function proportional to a regularization parameter. 
> 
>  add a term to the loss function proportional to the square of parameter coefficients. 
> 
>  add a term to the loss function proportional to the absolute value of parameter coefficients. 
> 
> Correct
> 
> Correct! You can find more information in the Regularization Techniques lesson.
> 
> ### 5.
> 
> Question 5
> 
> Compared with Lasso regression (assuming similar implementation), Ridge regression is:
> 
> 1 / 1 point
> 
>  less likely to overfit to training data. 
> 
>  more likely to overfit to training data. 
> 
      less likely to set feature coefficients to zero. 
> 
>  more likely to set feature coefficients to zero. 
> 
> Correct
> 
> Correct! You can find more information in the Regularization Techniques lesson.
>
> -- https://www.coursera.org/learn/supervised-machine-learning-regression/exam/LS3fq/check-for-understanding/attempt?redirectToCover=true#sSmi7T-legend
