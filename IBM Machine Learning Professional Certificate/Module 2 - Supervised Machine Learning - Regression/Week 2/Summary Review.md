# Summary/Review
> 
> ### **Training and Test Splits**
> 
> Splitting your data into a training and a test set can help you choose a model that has better chances at generalizing and is not overfitted.
> 
> The training data is used to fit the model, while the test data is used to measure error and performance. 
> 
> Training error tends to decrease with a more complex model.Cross validation error generally has a u-shape. It decreases with more complex models, up to a point in which it starts to increase again. 
> 
> ### **Cross Validation**
> 
> The three most common cross validation approaches are:
> 
> *   k-fold cross validation
> 
> *   leave one out cross validation
> 
> *   stratified cross validation
> 
> ### **Polynomial Regression**
> 
> Polynomial terms help you capture nonlinear effects of your features. 
> 
> Other algorithms that help you extend your linear models are:
> 
> *   Logistic Regression
> 
> *   K-Nearest Neighbors
> 
> *   Decision Trees
> 
> *   Support Vector Machines
> 
> *   Random Forests
> 
> *   Ensemble Methods
> 
> *   Deep Learning Approaches
>
> -- https://www.coursera.org/learn/supervised-machine-learning-regression/supplement/G6Oa5/summary-review#main
