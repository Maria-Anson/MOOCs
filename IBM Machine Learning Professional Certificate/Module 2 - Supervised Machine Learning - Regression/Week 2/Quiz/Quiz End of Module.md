# End of Module Quiz
> ### 1.
> 
> Question 1
> 
> The main purpose of splitting your data into a training and test sets is:
> 
> 1 / 1 point
> 
>  To improve accuracy 
> 
      To avoid overfitting 
> 
>  To improve regularization 
> 
>  To improve crossvalidation and overfitting 
> 
> Correct
> 
> Correct! You can find more information in the Training and Test Splits lesson.
> 
> ### 2.
> 
> Question 2
> 
> (True/False) For a dataset with _M_ observations and _N_ features, Stratified cross-validation is equivalent to k-fold cross-validation, where _k_ =_N-1_ .
> 
> 1 / 1 point
> 
>  True 
> 
      False 
> 
> Correct
> 
> Correct! You can find more information in the cross-validation lesson.
> 
> ### 3.
> 
> Question 3
> 
> (True/False) A linear regression model is being tested by cross-validation. Relative to K-fold cross-validation, stratified cross-validation (with the same _k_ ) will likely increase the variance of estimated parameters.
> 
> 1 / 1 point
> 
>  True 
> 
      False 
> 
> Correct
> 
> Correct! You can find more information in the cross-validation lesson.
> 
> ### 4.
> 
> Question 4
> 
> In K-fold cross-validation, how will increasing _k_ affect the variance (across subsamples) of estimated model parameters?
> 
> 1 / 1 point
> 
>  Increasing _k_ will not affect the variance of estimated parameters. 
> 
>  Increasing _k_ will usually reduce the variance of estimated parameters. 
> 
      Increasing _k_ will usually increase the variance of estimated parameters. 
> 
>  Increasing _k_ will increase the variance of estimated parameters if models are underfit, but reduce it if models are overfit. 
> 
> Correct
> 
> Correct! You can find more information in the cross-validation lesson.
>
> -- https://www.coursera.org/learn/supervised-machine-learning-regression/exam/zHRgJ/end-of-module-quiz/attempt?redirectToCover=true#Fgvm96-legend
