# Module 5 Graded Quiz
> ### 1.
> 
> Question 1
> 
> The term _Bagging_ stands for bootstrap aggregating.
> 
> 1 / 1 point
> 

      True 
> 
>  False 
> 
> Correct
> 
> Correct! You can find more information in the lesson: _Ensemble Based Methods and Bagging_.
> 
> ### 2.
> 
> Question 2
> 
> This is the best way to choose the number of trees to build on a Bagging ensemble.
> 
> 1 / 1 point
> 
>  Choose a large number of trees, typically above 100 
> 
>  Prioratize training error metrics over out of bag sample 
> 

      Tune number of trees as a hyperparameter that needs to be optimized 
> 
>  Choose a number of trees past the point of diminishing returns 
> 
> Correct
> 
> Correct! You can find more information in the lesson: _Ensemble Based Methods and Bagging_.
> 
> ### 3.
> 
> Question 3
> 
> Which type of Ensemble modeling approach is NOT a special case of model averaging?
> 
> 1 / 1 point
> 
>  The Bagging method of Bootstrap aggregation 
> 
>  Random Forest methods 
> 

      Boosting methods 
> 
>  The Pasting method of Bootstrap aggregation 
> 
> Correct
> 
> Correct! You can find more information in the lesson _Overview of Boosting._
> 
> ### 4.
> 
> Question 4
> 
> What is an ensemble model that needs you to look at out of bag error?
> 
> 1 / 1 point
> 
>  Logistic Regression. 
> 
>  Stacking 
> 
>  Out of Bag Regression 
> 

      Random Forest 
> 
> Correct
> 
> Correct! You can find more information in the lesson _Random Forest._
> 
> ### 5.
> 
> Question 5
> 
> What is the main condition to use stacking as ensemble method?
> 
> 1 / 1 point
> 
>  Models need to be nonparametric 
> 

      Models need to output predicted probabilities 
> 
>  Models need to output residual values for each class 
> 
>  Models need to be parametric 
> 
> Correct
> 
> Correct! You can find more information in the lesson _Stacking._
> 
> ### 6.
> 
> Question 6
> 
> This tree ensemble method only uses a subset of the features for each tree:
> 
> 1 / 1 point
> 
>  Adaboost 
> 
>  Stacking 
> 
>  Bagging 
> 

      Random Forest 
> 
> Correct
> 
> Correct! This tree ensemble only uses a subset of the features for each tree. For more information, please review the Random Forest lesson.
> 
> ### 7.
> 
> Question 7
> 
> Order these tree ensembles in order of most randomness to least randomness:
> 
> 1 / 1 point
> 

      Random Trees, Random Forest, Bagging 
> 
>  Bagging, Random Forest, Random Trees 
> 
>  Random Forest, Bagging, Random Trees 
> 
>  Random Forest, Random Trees, Bagging 
> 
> Correct
> 
> Correct! Random Trees add one more degree of randomness than Random Forests and two more than Bagging. You can find more information in the Random Forest lesson.
> 
> ### 8.
> 
> Question 8
> 
> This is an ensemble model that does not use bootstrapped samples to fit the base trees, takes residuals into account, and fits the base trees iteratively:
> 
> 1 / 1 point
> 
>  Bagging 
> 
>  Random Trees 
> 

      Boosting 
> 
>  Random Forest 
> 
> Correct
> 
> Correct! These are all characteristics of boosting algorithms. You can find more information in the _Boosting_ lesson.
> 
> ### 9.
> 
> Question 9
> 
> When comparing the two ensemble methods Bagging and Boosting, what is one characteristic of Boosting?
> 
> 1 / 1 point
> 

      Fits entire data set 
> 
>  Only data points are considered 
> 
>  Bootstraped samples 
> 
>  No weighting used 
> 
> Correct
> 
> Correct. With Boosting you can use the entire data set to train each of the classifiers
> 
> ### 10.
> 
> Question 10
> 
> What is the most frequently discussed loss function in boosting algorithms?
> 
> 1 / 1 point
> 

      Gradient Boosting Loss Function 
> 
>  AdaBoost Loss Function 
> 
>  0-1 Loss Function 
> 
>  Gradient Loss Function 
> 
> Correct
>
> -- https://www.coursera.org/learn/supervised-machine-learning-classification/exam/kvVLI/module-5-graded-quiz/view-attempt#0DghxZ-legend
