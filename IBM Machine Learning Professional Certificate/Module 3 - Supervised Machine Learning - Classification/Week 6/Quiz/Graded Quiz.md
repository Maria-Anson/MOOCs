# Module 6 Graded Quiz
> ### 1.
> 
> Question 1
> 
> Which of the following statements about Downsampling is TRUE?
> 
> 1 / 1 point
> 
>  Downsampling is likely to decrease Recall. 
> 
>  Downsampling results in excessive focus on the more frequently-occurring class. 
> 
>  Downsampling preserves all the original observations. 
> 

      Downsampling is likely to decrease Precision. 
> 
> Correct
> 
> Correct! You can find more information in the lesson _Upsampling and Downsampling_.
> 
> ### 2.
> 
> Question 2
> 
> Which of the following statements about Random Upsampling is TRUE?
> 
> 1 / 1 point
> 

      Random Upsampling preserves all original observations. 
> 
>  Random Upsampling results in excessive focus on the more frequently-occurring class. 
> 
>  Random Upsampling will generally lead to a higher F1 score. 
> 
>  Random Upsampling generates observations that were not part of the original data. 
> 
> Correct
> 
> Correct! You can find more information in the lesson _Upsampling and Downsampling_.
> 
> ### 3.
> 
> Question 3
> 
> Which of the following statements about Synthetic Upsampling is TRUE?
> 
> 1 / 1 point
> 
>  Synthetic Upsampling results in excessive focus on the more frequently-occurring class. 
> 
>  Synthetic Upsampling will generally lead to a higher F1 score. 
> 

      Synthetic Upsampling generates observations that were not part of the original data. 
> 
>  Synthetic Upsampling uses fewer hyperparameters than Random Upsampling. 
> 
> Correct
> 
> Correct! You can find more information in the lesson _Upsampling and Downsampling_.
> 
> ### 4.
> 
> Question 4
> 
> What can help humans to interpret the behaviors and methods of Machine Learning models more easily?
> 
> 1 / 1 point
> 
>  Model Debug 
> 
>  Explanation Debug 
> 

      Model Explanations 
> 
>  Model Trust 
> 
> Correct
> 
> Correct! Model explanations can help humans to interpret the behaviors and methods of Machine Learning models more easily
> 
> ### 5.
> 
> Question 5
> 
> What type of explanation method can be used to explain different types of Machine Learning models no matter the model structures and complexity?
> 
> 1 / 1 point
> 
>  Model Explanations 
> 
>  Local Interpretable Model-Agnostic Explanations (LIME) 
> 

      Model-Agnostic Explanations 
> 
>  Model Trust Explanations 
> 
> Correct
> 
> Correct! The Model-Agnostic explanation can be used to describe different types of Machine Learning models no matter the complexity while also having the same formats and presentations for model explanations?
> 
> ### 6.
> 
> Question 6
> 
> What reason might a Global Surrogate model fail?
> 
> 1 / 1 point
> 
>  Single data instance groups 
> 
>  Single clusters in the data instance groups 
> 

      Large inconsistency between surrogate models and black-box models 
> 
>  Consistency between surrogate models and black-box models 
> 
> Correct
> 
> Correct! A Global Surrogate model might fail if there is a large inconsistency between surrogate models and black-box models.
> 
> ### 7.
> 
> Question 7
> 
> When working with unbalanced sets, what should be done to the samples so the class balance remains consistent in both the train and test set?
> 
> 1 / 1 point
> 

      Stratify the samples 
> 
>  Use a combination of oversampling and undersampling 
> 
>  Apply weighted observations 
> 
>  Use oversampling 
> 
> Correct
> 
> Correct! You should stratify the samples so the class balance remains consistent in both the train and test set.
> 
> ### 8.
> 
> Question 8
> 
> What approach are you using when trying to increase the size of a minority class so that it is similar to the size of the majority class?
> 
> 1 / 1 point
> 
>  Random Oversampling 
> 
>  Undersampling 
> 

      Oversampling 
> 
>  Synthetic Oversampling 
> 
> Correct
> 
> Correct! You are oversampling when trying to increase the size of a minority class so that it is similar to the size of the majority class
> 
> ### 9.
> 
> Question 9
> 
> What approach are you using when you create a new sample of a minority class that does not yet exist?
> 
> 1 / 1 point
> 
>  Weighting 
> 
>  Oversampling 
> 
>  Random Oversampling 
> 

      Synthetic Oversampling 
> 
> Correct
> 
> Correct! Synthetic Oversampling is an approach used to create a new sample of a minority class that does not yet exist.
> 
> ### 10.
> 
> Question 10
> 
> What intuitive technique is used for unbalanced datasets that ensures a continuous downsample for each of the bootstrap samples?
> 
> 1 / 1 point
> 
>  Upsampling 
> 

      Blagging 
> 
>  Downsampling 
> 
>  SMOTE 
> 
> Correct
> 
> Correct! Blagging is an intuitive technique used for unbalanced datasets that ensures a continuous downsample for each of the bootstrap samples.
>
> -- https://www.coursera.org/learn/supervised-machine-learning-classification/exam/UdNql/module-6-graded-quiz/view-attempt#FHZVOc-legend
