## Latent Dirichlet Allocation
> 
> Total points 5
> 
> ### 1.
> 
> Question 1
> 
> (True/False) According to the assumptions of LDA, each document in the corpus contains words about a single topic.
> 
> 1 point
> 
>  True 
> 


    False 
> 
> ### 2.
> 
> Question 2
> 
> (True/False) Using LDA to analyze a set of documents is an example of a supervised learning task.
> 
> 1 point
> 
>  True 
> 

    False 
> 
> ### 3.
> 
> Question 3
> 
> (True/False) When training an LDA model, changing the ordering of words in a document does not affect the overall joint probability.
> 
> 1 point
> 

    True 
> 
>  False 
> 
> ### 4.
> 
> Question 4
> 
> (True/False) Suppose in a trained LDA model two documents have no topics in common (i.e., one document has 0 weight on any topic with non-zero weight in the other document). As a result, a single word in the vocabulary cannot have high probability of occurring in both documents.
> 
> 1 point
> 
>  True 
> 

    False 
> 
> ### 5.
> 
> Question 5
> 
> (True/False) Topic models are guaranteed to produce weights on words that are coherent and easily interpretable by humans.
> 
> 1 point
> 
>  True 
> 

    False
>
